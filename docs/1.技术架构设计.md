# 音视频转写与 AI 总结工具 - 技术架构设计

## 1. 总体架构

### 1.1 系统概述

本项目采用 **前后端分离架构**：
- **后端服务**: 运行在 Windows 主机上，提供 RESTful API，负责 AI 处理
- **前端应用**: 纯 Web 应用，通过浏览器访问，支持局域网内所有设备
- **部署方式**: 局域网部署，家庭网络使用

### 1.2 架构图

```
┌─────────────────────────────────────────────────────────────┐
│            客户端层（浏览器，任何设备）                          │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐  │
│  │         React Web Application                         │  │
│  │  ┌────────────┐  ┌──────────────┐  ┌─────────────┐  │  │
│  │  │  UI Pages  │  │ State Mgmt   │  │  API Client │  │  │
│  │  │  (React)   │  │  (Zustand)   │  │   (axios)   │  │  │
│  │  └────────────┘  └──────────────┘  └─────────────┘  │  │
│  └──────────────────────────────────────────────────────┘  │
└────────────────────────┬─────────────────────────────────────┘
                         │ HTTP/WebSocket
                         │ (局域网)
                         ▼
┌─────────────────────────────────────────────────────────────┐
│         服务端层（Windows + GPU，家庭服务器）                   │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐  │
│  │         API 服务器 (Node.js/Fastify)                  │  │
│  │  ┌───────────────┐  ┌──────────────┐  ┌──────────┐  │  │
│  │  │ File Upload   │  │ Task Queue   │  │ WebSocket│  │  │
│  │  │   Handler     │  │   Manager    │  │  Server  │  │  │
│  │  └───────────────┘  └──────────────┘  └──────────┘  │  │
│  └─────────────────────┬────────────────────────────────┘  │
│                        │                                    │
│  ┌─────────────────────▼────────────────────────────────┐  │
│  │         AI 处理层（Python Workers）                    │  │
│  │  ┌──────────────────┐  ┌──────────────────────────┐  │  │
│  │  │  ASR Service     │  │   LLM Service            │  │  │
│  │  │  faster-whisper  │  │   Ollama + Qwen3-14B     │  │  │
│  │  │  (GPU 加速)       │  │   (GPU 加速)              │  │  │
│  │  └──────────────────┘  └──────────────────────────┘  │  │
│  │  ┌──────────────────────────────────────────────────┐  │
│  │  │   音视频处理 (FFmpeg)                             │  │
│  │  └──────────────────────────────────────────────────┘  │
│  └──────────────────────────────────────────────────────┘  │
│                        │                                    │
│  ┌─────────────────────▼────────────────────────────────┐  │
│  │         数据存储层 (SQLite + 文件系统)                 │  │
│  │  ┌──────────────┐  ┌──────────────┐  ┌───────────┐  │  │
│  │  │   项目元数据  │  │  转写结果     │  │ 音视频文件 │  │  │
│  │  └──────────────┘  └──────────────┘  └───────────┘  │  │
│  └──────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
```

### 1.3 技术栈选型

#### 前端层（Web 应用）
- **框架**: React 18 + TypeScript 5.x
- **UI 组件库**: Ant Design / shadcn/ui
- **状态管理**: Zustand
- **样式方案**: Tailwind CSS
- **构建工具**: Vite
- **HTTP 客户端**: axios
- **实时通信**: WebSocket (Socket.io)

#### 后端层（API 服务器）
- **框架**: Fastify / Express (Node.js 18+)
- **语言**: TypeScript 5.x
- **文件上传**: @fastify/multipart
- **任务队列**: Bull / Bee-Queue
- **WebSocket**: Socket.io
- **进程管理**: PM2 (生产环境)

#### AI 处理层
- **运行时**: Python 3.10+
- **语音识别**: faster-whisper (GPU 加速)
- **LLM 推理**: Ollama + Qwen3-14B
- **音视频处理**: FFmpeg
- **进程通信**: stdin/stdout (JSON)

#### 数据存储层
- **数据库**: SQLite (better-sqlite3)
- **文件存储**: 本地文件系统
- **缓存**: 内存缓存 (可选 Redis)

#### 部署与运维
- **服务端OS**: Windows 10/11
- **进程管理**: PM2 / Windows Service
- **日志**: Winston / Pino
- **监控**: 简单的健康检查接口

## 2. 核心技术方案

### 2.1 语音识别（ASR）方案对比

#### 方案 A: OpenAI Whisper (faster-whisper)
**优势**:
- ✅ 业界最高精度（OpenAI 开源）
- ✅ 多语言支持优秀（支持 99 种语言）
- ✅ GPU 加速成熟（CUDA）
- ✅ 模型大小可选（tiny~large-v3）
- ✅ 社区活跃，文档完善

**劣势**:
- ❌ 需要 Python 运行时
- ❌ 首次加载模型较慢（~5-10秒）

**推荐配置**:
- 模型: `large-v3` (3GB) 或 `medium` (1.5GB)
- 引擎: `faster-whisper` (CTranslate2 优化，速度提升 4x)
- 量化: `int8` 量化（精度损失<1%，速度提升 2x）

**性能预估**（4070 TI Super）:
- large-v3 模型：15-20x 实时速度
- medium 模型：25-30x 实时速度

#### 方案 B: whisper.cpp
**优势**:
- ✅ C++ 实现，无 Python 依赖
- ✅ 集成更简单
- ✅ 内存占用更低
- ✅ 支持 CUDA 和 Metal (macOS)

**劣势**:
- ❌ 速度略慢于 faster-whisper
- ❌ 功能相对简单

**适用场景**: 希望减少依赖，牺牲一些性能

#### **推荐方案**: **faster-whisper + CUDA**
- 性能最优，精度最高
- 适合你的硬件配置（4070 TI Super CUDA 加速）

### 2.2 AI 总结（LLM）方案对比

#### 方案 A: 本地部署 LLM (Ollama)
**推荐模型**:
1. **Qwen3-14B** (首选)
   - 模型大小: 9GB (Q4量化)
   - 中文能力: ⭐⭐⭐⭐⭐
   - 速度: 35-45 tokens/s (4070 TI Super)
   - 特点: 阿里最新一代，逻辑推理和总结质量显著优于 7B/8B

2. **Llama 3.1-8B** (备选)
   - 模型大小: 4.7GB (Q4量化)
   - 英文能力: ⭐⭐⭐⭐⭐
   - 中文能力: ⭐⭐⭐⭐
   - 速度: 35-50 tokens/s

3. **Mistral-7B** (备选)
   - 模型大小: 4.1GB
   - 平衡性好，速度快

**技术实现**:
```typescript
// 使用 Ollama 作为本地 LLM 服务器
// Ollama 提供 REST API，易于集成
import axios from 'axios';

const response = await axios.post('http://localhost:11434/api/generate', {
  model: 'qwen3:14b',
  prompt: '请总结以下内容：...',
  stream: true
});
```

**优势**:
- ✅ 完全本地化，数据不外传
- ✅ 无 API 调用成本
- ✅ 响应速度快（GPU 加速）
- ✅ 可自定义 prompt
- ✅ 支持流式输出

**劣势**:
- ❌ 需要额外 8-10GB 磁盘空间
- ❌ 质量略逊于 GPT-4

#### 方案 B: llama.cpp
**优势**:
- ✅ C++ 实现，更轻量
- ✅ CUDA/Metal 支持
- ✅ 社区活跃

**劣势**:
- ❌ 集成相对复杂
- ❌ 需要手动管理模型

#### 方案 C: 混合方案（本地 + 云端可选）
- 默认本地 LLM
- 提供云端 API 选项（OpenAI/Claude）作为高级功能
- 用户可自由选择

#### **推荐方案**: **Ollama + Qwen3-14B**
- 易于安装和管理
- 中文能力极强，逻辑推理能力显著优于 7B/8B
- GPU 加速效率高（串行执行模式，16GB 显存完全够用）
- REST API 集成简单

### 2.3 技术架构详细设计

#### 2.3.1 API 端点设计

**核心 API 端点**:

```typescript
// POST /api/projects - 创建新项目（上传文件）
// GET  /api/projects - 获取项目列表
// GET  /api/projects/:id - 获取项目详情
// DELETE /api/projects/:id - 删除项目

// POST /api/projects/:id/transcribe - 开始转写
// GET  /api/projects/:id/transcription - 获取转写结果
// POST /api/projects/:id/summarize - 生成总结

// WebSocket /ws - 实时进度推送
```

#### 2.3.2 文件上传与处理流程

```typescript
// 后端：文件上传处理
import { FastifyRequest } from 'fastify';
import { pipeline } from 'stream/promises';

async function handleFileUpload(request: FastifyRequest) {
  const data = await request.file();

  // 1. 保存上传的文件
  const projectId = generateId();
  const filePath = path.join(UPLOAD_DIR, projectId, data.filename);

  await pipeline(data.file, fs.createWriteStream(filePath));

  // 2. 创建项目记录
  const project = await db.createProject({
    id: projectId,
    name: data.filename,
    sourceFile: filePath,
    status: 'pending'
  });

  // 3. 提取音频（异步）
  taskQueue.add('extract-audio', { projectId });

  return { projectId, status: 'uploaded' };
}

// 处理流程（任务队列）
async function processProject(projectId: string) {
  // 1. 提取音频
  const audioPath = await extractAudio(project.sourceFile);

  // 2. 发送到 Python Worker 转写
  const transcription = await transcribeAudio(audioPath, (progress) => {
    // 通过 WebSocket 推送进度
    io.to(projectId).emit('progress', { stage: 'transcribing', progress });
  });

  // 3. 保存结果
  await db.saveTranscription(projectId, transcription);

  // 4. 通知完成
  io.to(projectId).emit('completed', { projectId });
}
```

**FFmpeg 音频提取**:
```typescript
import ffmpeg from 'fluent-ffmpeg';

function extractAudio(videoPath: string): Promise<string> {
  const outputPath = `${tempDir}/${uuid()}.wav`;

  return new Promise((resolve, reject) => {
    ffmpeg(videoPath)
      .audioChannels(1)           // 单声道
      .audioFrequency(16000)      // 16kHz (Whisper 标准)
      .audioCodec('pcm_s16le')    // PCM 格式
      .format('wav')
      .on('end', () => resolve(outputPath))
      .on('error', reject)
      .save(outputPath);
  });
}
```

#### 2.3.2 Python Worker 架构

```python
# python_worker/asr_service.py
from faster_whisper import WhisperModel
import json
import sys

class ASRService:
    def __init__(self):
        # 使用 CUDA 加速
        self.model = WhisperModel(
            "large-v3",
            device="cuda",
            compute_type="int8_float16"  # 混合精度
        )

    def transcribe(self, audio_path: str, language: str = None):
        segments, info = self.model.transcribe(
            audio_path,
            language=language,
            beam_size=5,
            vad_filter=True,  # 语音活动检测
            vad_parameters={
                "threshold": 0.5,
                "min_speech_duration_ms": 250
            }
        )

        results = []
        for segment in segments:
            results.append({
                "start": segment.start,
                "end": segment.end,
                "text": segment.text,
                "confidence": segment.avg_logprob
            })

        return {
            "language": info.language,
            "segments": results
        }

# 通过 stdin/stdout 与 Node.js 通信
if __name__ == "__main__":
    service = ASRService()

    for line in sys.stdin:
        request = json.loads(line)
        result = service.transcribe(request['audio_path'])
        print(json.dumps(result), flush=True)
```

**Node.js 端调用**:
```typescript
import { spawn } from 'child_process';

class PythonWorkerManager {
  private process: ChildProcess;

  constructor() {
    // 启动 Python 进程
    this.process = spawn('python', ['python_worker/asr_service.py']);
  }

  async transcribe(audioPath: string): Promise<TranscriptionResult> {
    return new Promise((resolve, reject) => {
      const request = { audio_path: audioPath };

      this.process.stdout.once('data', (data) => {
        const result = JSON.parse(data.toString());
        resolve(result);
      });

      this.process.stdin.write(JSON.stringify(request) + '\n');
    });
  }
}
```

#### 2.3.3 前端架构设计

**API 客户端封装**:
```typescript
// services/api.ts
import axios from 'axios';

const API_BASE_URL = process.env.VITE_API_URL || 'http://localhost:8080';

export const api = axios.create({
  baseURL: API_BASE_URL,
  timeout: 30000,
});

// 上传文件
export async function uploadFile(file: File, onProgress?: (progress: number) => void) {
  const formData = new FormData();
  formData.append('file', file);

  const response = await api.post('/api/projects', formData, {
    headers: { 'Content-Type': 'multipart/form-data' },
    onUploadProgress: (event) => {
      if (event.total) {
        const progress = (event.loaded / event.total) * 100;
        onProgress?.(progress);
      }
    }
  });

  return response.data;
}

// 开始转写
export async function startTranscription(projectId: string) {
  const response = await api.post(`/api/projects/${projectId}/transcribe`);
  return response.data;
}

// 获取转写结果
export async function getTranscription(projectId: string) {
  const response = await api.get(`/api/projects/${projectId}/transcription`);
  return response.data;
}
```

**WebSocket 实时通信**:
```typescript
// services/websocket.ts
import { io, Socket } from 'socket.io-client';

class WebSocketService {
  private socket: Socket | null = null;

  connect() {
    this.socket = io('http://localhost:8080');

    this.socket.on('connect', () => {
      console.log('WebSocket connected');
    });
  }

  // 订阅项目进度
  subscribeToProject(projectId: string, onProgress: (data: any) => void) {
    this.socket?.emit('subscribe', { projectId });
    this.socket?.on('progress', onProgress);
  }

  unsubscribeFromProject(projectId: string) {
    this.socket?.emit('unsubscribe', { projectId });
    this.socket?.off('progress');
  }
}

export const wsService = new WebSocketService();
```

**状态管理 (Zustand)**:
```typescript
// stores/useAppStore.ts
import create from 'zustand';
import { uploadFile, startTranscription } from '@/services/api';
import { wsService } from '@/services/websocket';

interface AppState {
  projects: Project[];
  currentProject: Project | null;
  processing: boolean;
  progress: number;

  // Actions
  uploadProject: (file: File) => Promise<void>;
  transcribeProject: (projectId: string) => Promise<void>;
}

export const useAppStore = create<AppState>((set, get) => ({
  projects: [],
  currentProject: null,
  processing: false,
  progress: 0,

  uploadProject: async (file) => {
    set({ processing: true, progress: 0 });

    const result = await uploadFile(file, (progress) => {
      set({ progress: progress * 0.2 }); // 上传占 20%
    });

    set({
      projects: [...get().projects, result],
      processing: false,
      progress: 100
    });
  },

  transcribeProject: async (projectId) => {
    set({ processing: true, progress: 0 });

    // 订阅进度更新
    wsService.subscribeToProject(projectId, (data) => {
      set({ progress: data.progress });
    });

    // 开始转写
    await startTranscription(projectId);
  }
}));
```

## 3. 数据存储设计

### 3.1 数据模型

```typescript
interface Project {
  id: string;
  name: string;
  sourceFile: string;        // 原始文件路径
  audioFile?: string;        // 提取的音频文件
  createdAt: Date;
  updatedAt: Date;
  status: 'pending' | 'processing' | 'completed' | 'error';
  transcription?: Transcription;
  summary?: Summary;
  tags: string[];
  metadata: {
    duration: number;
    language: string;
    fileSize: number;
  };
}

interface Transcription {
  language: string;
  segments: TranscriptionSegment[];
  fullText: string;
}

interface TranscriptionSegment {
  id: string;
  start: number;
  end: number;
  text: string;
  confidence: number;
  speaker?: string;  // 说话人标识（未来功能）
}

interface Summary {
  abstract: string;          // 摘要
  keyPoints: string[];       // 要点
  entities: {                // 实体识别
    people: string[];
    places: string[];
    dates: string[];
  };
  topics: string[];          // 主题标签
  generatedAt: Date;
}
```

### 3.2 存储方案

**本地数据库: SQLite**
```sql
-- projects 表
CREATE TABLE projects (
  id TEXT PRIMARY KEY,
  name TEXT NOT NULL,
  source_file TEXT NOT NULL,
  audio_file TEXT,
  status TEXT NOT NULL,
  created_at DATETIME,
  updated_at DATETIME,
  metadata TEXT  -- JSON 格式
);

-- transcriptions 表
CREATE TABLE transcriptions (
  id TEXT PRIMARY KEY,
  project_id TEXT REFERENCES projects(id),
  language TEXT,
  full_text TEXT,
  segments TEXT  -- JSON 格式
);

-- summaries 表
CREATE TABLE summaries (
  id TEXT PRIMARY KEY,
  project_id TEXT REFERENCES projects(id),
  abstract TEXT,
  key_points TEXT,  -- JSON 数组
  entities TEXT,    -- JSON 对象
  generated_at DATETIME
);
```

**大文件存储**: 文件系统
- 音视频文件：保持原始位置或复制到应用数据目录
- 提取的音频：临时目录，处理完成后可选择删除

## 4. 部署架构

### 4.1 服务端部署（Windows）

#### 系统要求
- **操作系统**: Windows 10/11 (x64)
- **硬件**: i5-13600KF + RTX 4070 TI Super（或同等配置）
- **软件依赖**:
  - Node.js 18+
  - Python 3.10+
  - CUDA 11.8+ & cuDNN
  - FFmpeg
  - Ollama

#### 部署方式

**方式 1: 开发模式**
```bash
# 后端
cd server
npm install
npm run dev

# 前端
cd web
npm install
npm run dev
```

**方式 2: 生产模式（PM2）**
```bash
# 安装 PM2
npm install -g pm2

# 构建前端
cd web
npm run build

# 启动后端（自动服务前端静态文件）
cd server
pm2 start ecosystem.config.js

# 开机自启动
pm2 startup
pm2 save
```

**方式 3: Windows 服务**
```bash
# 使用 node-windows 创建 Windows 服务
npm install -g node-windows
node install-service.js
```

### 4.2 客户端访问

#### 浏览器兼容性
- Chrome 90+
- Edge 90+
- Firefox 88+
- Safari 14+

#### 访问方式
- **本机访问**: `http://localhost:8080`
- **局域网访问**: `http://192.168.x.x:8080`
  - 需要在 Windows 防火墙中开放 8080 端口
  - 或使用自定义域名（配置 hosts 文件）

#### 响应式设计
- 桌面端（>1024px）: 完整功能
- 平板（768-1024px）: 优化布局
- 手机（<768px）: 简化界面（查看和基本操作）

## 5. 性能优化策略

### 5.1 GPU 加速优化
- 使用 CUDA 11.8+ 和 cuDNN 8.x
- 模型量化（int8/float16 混合精度）
- 批处理优化（如果处理多个文件）
- 动态显存管理

### 5.2 多线程/多进程设计
- **主进程**: UI 和业务逻辑
- **Python Worker**: 独立进程处理 AI 任务
- **后台队列**: 支持多任务并发处理

### 5.3 缓存策略
- 模型缓存：首次加载后保持在内存
- 音频缓存：提取的音频文件缓存
- 结果缓存：转写和总结结果持久化

### 5.4 内存管理
- 大文件流式处理（避免一次性加载）
- 及时释放不需要的资源
- 监控内存使用，超限时警告

## 6. 安全与隐私

### 6.1 数据隐私
- ✅ 所有处理完全本地化
- ✅ 不连接外部服务器（可选云端 API 需用户明确授权）
- ✅ 敏感数据加密存储（可选功能）

### 6.2 文件权限
- 最小权限原则
- 用户明确授权访问文件

## 7. 安全与网络

### 7.1 网络安全（局域网）

由于是家庭局域网使用，安全策略保持简单实用：

**基础安全措施**:
```typescript
// 简单的 token 认证（可选）
app.post('/api/auth/login', (req, res) => {
  const { password } = req.body;
  if (password === process.env.ADMIN_PASSWORD) {
    const token = generateToken();
    res.json({ token });
  }
});

// 中间件：验证 token
app.use('/api/*', (req, res, next) => {
  const token = req.headers.authorization;
  if (!token && process.env.REQUIRE_AUTH === 'true') {
    return res.status(401).json({ error: 'Unauthorized' });
  }
  next();
});
```

**文件上传限制**:
- 最大文件大小: 5GB (可配置)
- 允许的文件类型: 音视频格式白名单
- 上传频率限制: 防止滥用

**防火墙配置**:
```bash
# Windows 防火墙规则
netsh advfirewall firewall add rule name="MiaoJi Server" dir=in action=allow protocol=TCP localport=8080
```

### 7.2 数据隐私
- ✅ 所有 AI 处理完全在本地服务器完成
- ✅ 数据不出局域网，不上传云端
- ✅ 用户可随时删除项目和文件
- ✅ 无第三方追踪或分析

## 8. 开发计划

### 阶段 1: 环境搭建与验证（1-2 周）
- [ ] 开发环境配置（Node.js, Python, CUDA）
- [ ] 核心技术验证（faster-whisper, Ollama）
- [ ] 项目结构初始化（Monorepo）
- [ ] 基础 API 框架搭建（Fastify）
- [ ] React 前端脚手架创建

### 阶段 2: MVP 开发（3-4 周）
- [ ] 文件上传功能（后端 + 前端）
- [ ] FFmpeg 音频提取
- [ ] Python Worker 集成（faster-whisper）
- [ ] 任务队列系统
- [ ] WebSocket 实时通信
- [ ] 基础转写 UI
- [ ] SQLite 数据库实现

### 阶段 3: LLM 集成（1-2 周）
- [ ] Ollama 服务集成
- [ ] AI 总结 API 实现
- [ ] 总结结果展示 UI
- [ ] 流式输出支持

### 阶段 4: 功能完善（2-3 周）
- [ ] 项目列表与管理
- [ ] 转写结果编辑器
- [ ] 多格式导出功能
- [ ] 音频同步播放
- [ ] 响应式 UI 优化

### 阶段 5: 部署与优化（1-2 周）
- [ ] PM2 生产部署配置
- [ ] Windows 服务安装脚本
- [ ] 性能优化（GPU 调优）
- [ ] 局域网测试（多设备）
- [ ] 用户文档编写

**预计总开发时间**: 8-13 周（2-3 个月）

## 9. 项目结构

```
miaoji/
├── server/                    # 后端服务
│   ├── src/
│   │   ├── api/              # API 路由
│   │   ├── services/         # 业务逻辑
│   │   ├── workers/          # Python Worker 管理
│   │   ├── database/         # 数据库
│   │   └── utils/            # 工具函数
│   ├── python/               # Python AI 模块
│   │   ├── asr_worker.py
│   │   └── requirements.txt
│   ├── uploads/              # 上传文件目录
│   ├── models/               # AI 模型缓存
│   └── package.json
│
├── web/                      # 前端应用
│   ├── src/
│   │   ├── components/       # React 组件
│   │   ├── pages/            # 页面
│   │   ├── services/         # API 客户端
│   │   ├── stores/           # 状态管理
│   │   └── utils/            # 工具函数
│   ├── public/
│   └── package.json
│
├── docs/                     # 文档
│   ├── 0.需求分析.md
│   └── 1.技术架构设计.md
│
├── scripts/                  # 部署脚本
│   ├── install-service.js
│   └── setup.sh
│
└── README.md
```

## 10. 风险评估与缓解

### 技术风险
| 风险 | 影响 | 缓解措施 |
|------|------|---------|
| Python 环境配置复杂 | 中 | 提供详细安装脚本，使用虚拟环境 |
| 大文件上传超时 | 中 | 增加超时配置，支持断点续传 |
| GPU 驱动不兼容 | 低 | 提供 CPU 降级方案 |
| 局域网 IP 变化 | 低 | 支持域名配置，提供自动发现 |

### 性能风险
| 风险 | 影响 | 缓解措施 |
|------|------|---------|
| 长音频处理时间过长 | 中 | 分段处理 + 实时进度提示 |
| 多用户并发冲突 | 低 | 任务队列排队处理 |
| 内存占用过高 | 中 | 流式处理 + 内存监控 |

## 11. 总结

### 技术栈总结
- **后端**: Fastify + TypeScript + Node.js
- **前端**: React + TypeScript + Tailwind CSS
- **ASR**: faster-whisper (large-v3) + CUDA
- **LLM**: Ollama + Qwen3-14B + CUDA
- **音视频**: FFmpeg
- **数据库**: SQLite
- **通信**: REST API + WebSocket
- **部署**: PM2 / Windows Service

### 核心优势
1. ✅ **前后端分离**: 架构清晰，维护简单
2. ✅ **无需安装客户端**: 浏览器直接访问，降低使用门槛
3. ✅ **局域网共享**: 多设备共享同一个 GPU 服务器
4. ✅ **完全本地化**: 数据不出局域网，隐私安全
5. ✅ **充分利用 GPU**: RTX 4070 TI Super 高性能处理
6. ✅ **零运营成本**: 无 API 调用费用
7. ✅ **易于扩展**: 未来可单独开发移动端 App

### 预期效果
- **转写速度**: 15-20x 实时速度（10分钟音频 < 40秒）
- **AI 总结速度**: 50-60 tokens/s
- **识别准确率**: ≥95%（中英文标准语音）
- **并发支持**: 3-5 个设备同时访问
- **响应时间**: API 响应 < 100ms（不含AI处理）
- **文件上传**: 局域网 50-100MB/s

### 与原 Electron 方案对比

| 维度 | Web 应用方案 | Electron 方案 |
|------|-------------|--------------|
| 客户端安装 | ✅ 无需安装 | ❌ 需要安装 |
| macOS 支持 | ✅ 浏览器即可 | ⚠️ 需要打包 |
| 开发复杂度 | ✅ 较低 | ⚠️ 较高 |
| 多设备共享 | ✅ 天然支持 | ❌ 不支持 |
| 安装包大小 | ✅ 无客户端 | ❌ 500MB+ |
| 更新维护 | ✅ 服务端统一更新 | ⚠️ 每台设备单独更新 |
| 性能 | ✅ 服务端 GPU | ✅ 本地 GPU |
| 离线使用 | ⚠️ 需局域网 | ✅ 完全离线 |

**结论**: Web 应用方案在用户体验、维护成本、多设备支持等方面优势明显，非常适合家庭局域网场景。
