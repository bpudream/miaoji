# 新机拉取后配置清单

> 从新电脑上 `git clone` 或 `git pull` 本项目后，按本文清单配置即可跑起来（开发模式 / 生产模式）。

---

## 一、仓库里有什么 / 没有什么

### 已在仓库中（拉取即有）

- 前后端源码：`server/`、`web/`
- 环境模板：`server/env.template`、`web/env.template`
- Python 依赖声明：`server/python/requirements.txt`（仅 `faster-whisper`）
- Nginx 示例配置：`nginx/miaoji.conf`、`nginx/nginx.conf.example`
- 文档：`docs/`（含开发启动指南、环境配置指南等）

### 不在仓库中（需自行准备）

| 类型 | 说明 | 处理方式 |
|------|------|----------|
| **环境变量** | `.env`、`.env.*` | 从 `env.template` 复制并填写 |
| **Node 依赖** | `node_modules/` | 在 `server`、`web` 下执行 `npm install` |
| **Python 虚拟环境** | `server/python/.venv/` | 创建 venv 并 `pip install -r requirements.txt` |
| **Whisper 模型** | `models/` 目录 | 自行下载或指定 `MODEL_PATH`，体积较大（约数 GB） |
| **数据库/上传目录** | `server/data/*.db`、`server/uploads/*` | 首次运行时会自动创建（需目录存在或可创建） |
| **构建产物** | `server/dist/`、`web/dist/` | 本地执行 `npm run build` 生成 |

---

## 二、系统与环境要求

### 必装（仅跑通前后端）

| 组件 | 版本/要求 | 校验命令 |
|------|-----------|----------|
| **Node.js** | 18+ | `node -v` |
| **npm** | 随 Node 安装 | `npm -v` |
| **Python** | 3.9+（推荐 3.10–3.12） | `python --version` |
| **FFmpeg** | 已加入 PATH | `ffmpeg -version` |

### 可选（完整 AI 能力：转写 + 总结）

| 组件 | 说明 | 参考文档 |
|------|------|----------|
| **CUDA + cuDNN** | GPU 加速 faster-whisper | `docs/kb/环境配置指南.md` |
| **Ollama** | 本地 LLM（总结等） | 安装后 `ollama pull qwen3:14b` |
| **Whisper 模型** | 如 `large-v3`，放于 `models/large-v3` 或自定义路径 | 在 `.env` 中配置 `MODEL_PATH` |

---

## 三、最小配置（只跑起来）

适合：新机拉代码后，先能访问前端、后端健康检查通过，不要求转写/总结功能。

### 1. 克隆/拉取代码

```powershell
git clone <repo-url> miaoji
cd miaoji
# 或已有仓库：git pull
```

### 2. 配置环境变量

**后端**

```powershell
cd server
copy env.template .env
```

编辑 `server/.env`，至少保留（开发默认即可）：

```bash
BACKEND_PORT=3000
# 其余可选；不配则用模板或代码内默认值
```

**前端**

```powershell
cd web
copy env.template .env
```

编辑 `web/.env`：

```bash
VITE_BACKEND_URL=http://localhost:3000
```

### 3. 安装依赖并启动

**后端**

```powershell
cd server
npm install
npm run dev
```

**Python Worker（转写依赖，若暂不做转写可先跳过）**

```powershell
cd server\python
python -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt
```

**前端（新开终端）**

```powershell
cd web
npm install
npm run dev
```

### 4. 访问与验证

- 前端：<http://localhost:13737>
- 后端健康检查：`curl http://localhost:3000/api/health` 应返回 `{"status":"ok",...}`

至此，项目已能在新机上“跑起来”；未配置模型/Ollama 时，转写和 AI 总结功能会不可用或报错，属预期。

---

## 四、完整配置（转写 + AI 总结可用）

在「最小配置」基础上，增加以下内容即可。

### 1. 系统级依赖（按需）

- **FFmpeg**：音视频处理，必须；未装则转写会失败。
- **CUDA 12.6 + cuDNN 9.x**（GPU 转写）：见 `docs/kb/环境配置指南.md`，含 zlibwapi.dll 等。
- **Ollama**：安装后执行 `ollama pull qwen3:14b`（或项目要求的其他模型）。

### 2. 后端 .env 建议

在 `server/.env` 中可明确指定（路径按本机实际修改）：

```bash
BACKEND_PORT=3000
PYTHON_WORKER_PATH=python/worker.py
PYTHON_PATH=python\.venv\Scripts\python.exe
MODEL_PATH=../models/large-v3
OLLAMA_HOST=http://127.0.0.1:11434
```

- `MODEL_PATH`：Whisper 模型目录（需自行下载或从别处拷贝到 `models/large-v3` 等）。
- `OLLAMA_HOST`：Ollama 未改端口则保持 `http://127.0.0.1:11434`。

### 3. 一键安装脚本（可选）

在 **server 目录或 release 根目录** 执行（脚本会找 `server`）：

```powershell
scripts\install-dependencies.bat
```

会执行：`npm install`（在 server 下）、在 `server/python` 下创建 `.venv` 并安装 `requirements.txt`。需本机已装好 Node、Python。

---

## 五、生产模式（Nginx 统一入口）

若要用 Nginx 提供前端 + 反向代理后端：

1. **构建前端**  
   `cd web && npm run build`，得到 `web/dist/`。

2. **后端端口与 Nginx 一致**  
   在 `server/.env` 中设置 `BACKEND_PORT=13636`（与仓库中 `nginx/miaoji.conf` 一致）。

3. **Nginx 配置**  
   - 将 `nginx/miaoji.conf` 拷贝到 Nginx 的 `conf` 目录（如 `D:\nginx\conf\`）。
   - 在 `conf/nginx.conf` 的 `http` 块中加入：`include miaoji.conf;`
   - 修改 `miaoji.conf` 中的 `root`、`alias` 为本机实际路径（例如项目在 `D:\miaoji` 则改为 `D:/miaoji/web/dist` 等）。

4. **启动**  
   先启动后端（`cd server && npm run build && npm start` 或 `npm run dev` 且端口 13636），再启动 Nginx。

5. **访问**  
   `http://localhost/miaoji`（或本机 IP）。

详见 `docs/开发启动指南.md` 中「生产模式（Nginx）」一节。

---

## 六、清单速查表

| 步骤 | 操作 | 必须？ |
|------|------|--------|
| 1 | 拉取代码 `git clone` / `git pull` | ✅ |
| 2 | 后端：`copy server\env.template server\.env`，并设 `BACKEND_PORT` | ✅ |
| 3 | 前端：`copy web\env.template web\.env`，并设 `VITE_BACKEND_URL` | ✅ |
| 4 | `cd server && npm install` | ✅ |
| 5 | `cd web && npm install` | ✅ |
| 6 | `cd server\python && python -m venv .venv && .venv\Scripts\pip install -r requirements.txt` | 做转写时 ✅ |
| 7 | 准备 Whisper 模型并配置 `MODEL_PATH` | 做转写时 ✅ |
| 8 | 安装并配置 Ollama（含模型） | 做总结时 ✅ |
| 9 | 安装 FFmpeg 并加入 PATH | 做转写时 ✅ |
| 10 | （生产）构建 web、配置 Nginx、后端端口 13636 | 仅生产 ✅ |

---

## 七、参考文档

- **开发/生产怎么启动**：`docs/开发启动指南.md`
- **GPU/CUDA/Ollama/Python 环境**：`docs/kb/环境配置指南.md`
- **后端安装与 Windows 服务**：`docs/kb/后端服务安装指南.md`
- **项目概述与端口说明**：根目录 `README.md`

---

*文档维护：随项目变更可更新此清单与上述文档的引用。*
